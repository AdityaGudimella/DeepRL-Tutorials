{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow with Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya-ws/anaconda3/envs/scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/aditya-ws/workspace/personal/thirdparty/baselines\")\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "from utils.wrappers import *\n",
    "from agents.DQN import Model as DQN_Agent\n",
    "from utils.ReplayMemory import PrioritizedReplayMemory\n",
    "from networks.layers import NoisyLinear\n",
    "\n",
    "from utils.hyperparameters import Config\n",
    "\n",
    "from morl import memories\n",
    "from morl import experiences as exp\n",
    "from morl import external_utils as extu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Multi-step returns\n",
    "config.N_STEPS = 3\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA=0.99\n",
    "config.LR=1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1000\n",
    "config.EXP_REPLAY_SIZE = 100000\n",
    "config.BATCH_SIZE = 32\n",
    "config.PRIORITY_ALPHA=0.3\n",
    "config.PRIORITY_BETA_START=0.4\n",
    "config.PRIORITY_BETA_FRAMES = 100000\n",
    "\n",
    "#epsilon variables\n",
    "config.SIGMA_INIT=0.5\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 10000\n",
    "config.MAX_FRAMES=1000000\n",
    "\n",
    "#Quantile Regression Parameters\n",
    "config.QUANTILES=51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQRDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, sigma_init=0.5, quantiles=51):\n",
    "        super(DuelingQRDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.quantiles=quantiles\n",
    "        self.head = torch.nn.Sequential(\n",
    "            extu.AsDtype(torch.float32),\n",
    "            extu.PixelNormalize()\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.adv1 = NoisyLinear(self.feature_size(), 512, sigma_init)\n",
    "        self.adv2 = NoisyLinear(512, self.num_actions*self.quantiles, sigma_init)\n",
    "\n",
    "        self.val1 = NoisyLinear(self.feature_size(), 512, sigma_init)\n",
    "        self.val2 = NoisyLinear(512, 1*self.quantiles, sigma_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        adv = F.relu(self.adv1(x))\n",
    "        adv = self.adv2(adv).view(-1, self.num_actions, self.quantiles)\n",
    "\n",
    "        val = F.relu(self.val1(x))\n",
    "        val = self.val2(val).view(-1, 1, self.quantiles)\n",
    "\n",
    "        return val + adv - adv.mean(dim=1).view(-1, 1, self.quantiles)\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        self.adv1.sample_noise()\n",
    "        self.adv2.sample_noise()\n",
    "        self.val1.sample_noise()\n",
    "        self.val2.sample_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(DQN_Agent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None):\n",
    "        self.num_quantiles = config.QUANTILES\n",
    "        self.cumulative_density = torch.tensor((2 * np.arange(self.num_quantiles) + 1) / (2.0 * self.num_quantiles), device=config.device, dtype=torch.float) \n",
    "        self.quantile_weight = 1.0 / self.num_quantiles\n",
    "\n",
    "        super(Model, self).__init__(static_policy, env, config)\n",
    "\n",
    "        self.nsteps=max(self.nsteps, 3)\n",
    "    \n",
    "    def declare_networks(self):\n",
    "        self.model = DuelingQRDQN(self.num_feats, self.num_actions, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n",
    "        self.target_model = DuelingQRDQN(self.num_feats, self.num_actions, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n",
    "        \n",
    "    def declare_memory(self):\n",
    "        self.memory = PrioritizedReplayMemory(self.experience_replay_size, self.priority_alpha, self.priority_beta_start, self.priority_beta_frames)\n",
    "        self.morl_memory = memories.NumbaPrioNStepExperienceReplay.with_numba_atari_buffer_and_prioritized_weights(\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            capacity_per_env=config.EXP_REPLAY_SIZE,\n",
    "            n_steps=3,\n",
    "            n_envs=1,\n",
    "            frame_stack=4,\n",
    "            store_last_state_only=True,\n",
    "        )\n",
    "\n",
    "    def morl_next_distribution(self, tensor_exp: exp.TensorExperience) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            self.target_model.sample_noise()\n",
    "            max_next_action = self.get_max_next_state_action(tensor_exp.next_states)\n",
    "            quantiles_next = self.target_model(tensor_exp.next_states).gather(1, max_next_action).squeeze(1)\n",
    "            return (\n",
    "                tensor_exp.rewards\n",
    "                + (self.gamma ** self.nsteps) * quantiles_next * (~tensor_exp.dones).float()\n",
    "            )\n",
    "        \n",
    "    def next_distribution(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_next = torch.zeros((self.batch_size, self.num_quantiles), device=self.device, dtype=torch.float)\n",
    "            if not empty_next_state_values:\n",
    "                self.target_model.sample_noise()\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                quantiles_next[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action).squeeze(dim=1)\n",
    "\n",
    "            quantiles_next = batch_reward + ((self.gamma**self.nsteps)*quantiles_next)\n",
    "\n",
    "        return quantiles_next\n",
    "\n",
    "    def morl_compute_loss(self, tensor_exp: exp.TensorExperience) -> torch.Tensor:\n",
    "        actions = tensor_exp.actions.long().unsqueeze(-1).expand(-1, -1, self.num_quantiles)\n",
    "        # weights = core.as_default_tensor(tensor_exp.weights)\n",
    "        weights = tensor_exp.weights\n",
    "        \n",
    "        self.model.sample_noise()\n",
    "        quantiles = self.model(tensor_exp.states)\n",
    "        quantiles = quantiles.gather(1, actions).squeeze(dim=1)\n",
    "        quantiles_next = self.morl_next_distribution(tensor_exp)\n",
    "\n",
    "        diff = quantiles_next.t().unsqueeze(-1) - quantiles.unsqueeze(0)\n",
    "\n",
    "        loss = self.huber(diff) * torch.abs(self.cumulative_density.view(1, -1) - (diff < 0).to(torch.float))\n",
    "        loss = loss.transpose(0,1)\n",
    "        self.morl_memory.update_priorities_(tensor_exp.indices, loss.detach().mean(1).sum(-1).abs().cpu().numpy().squeeze())\n",
    "        loss = loss * weights.view(self.batch_size, 1, 1)\n",
    "        loss = loss.mean(1).sum(-1).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.num_quantiles)\n",
    "\n",
    "        self.model.sample_noise()\n",
    "        quantiles = self.model(batch_state)\n",
    "        quantiles = quantiles.gather(1, batch_action).squeeze(1)\n",
    "\n",
    "        quantiles_next = self.next_distribution(batch_vars)\n",
    "          \n",
    "        diff = quantiles_next.t().unsqueeze(-1) - quantiles.unsqueeze(0)\n",
    "\n",
    "        loss = self.huber(diff) * torch.abs(self.cumulative_density.view(1, -1) - (diff < 0).to(torch.float))\n",
    "        loss = loss.transpose(0,1)\n",
    "        self.memory.update_priorities(indices, loss.detach().mean(1).sum(-1).abs().cpu().numpy().tolist())\n",
    "        loss = loss * weights.view(self.batch_size, 1, 1)\n",
    "        loss = loss.mean(1).sum(-1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, s):\n",
    "        with torch.no_grad():\n",
    "            # X = torch.tensor([s], device=self.device, dtype=torch.float) \n",
    "            X = torch.tensor(s, device=self.device, dtype=torch.float).div(255) \n",
    "            self.model.sample_noise()\n",
    "            a = (self.model(X) * self.quantile_weight).sum(dim=2).max(dim=1)[1]\n",
    "            # return a.item()\n",
    "            return a.view(-1, 1).cpu().numpy()\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        next_dist = self.model(next_states) * self.quantile_weight\n",
    "        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.num_quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, sigma, elapsed_time):\n",
    "    print('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    return\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    plt.plot(rewards)\n",
    "    if losses:\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "    if sigma:\n",
    "        plt.subplot(133)\n",
    "        plt.title('noisy param magnitude')\n",
    "        plt.plot(sigma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya-ws/anaconda3/envs/scratch/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:228: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  interpolation: int = Image.BILINEAR,\n",
      "/home/aditya-ws/anaconda3/envs/scratch/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:295: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  interpolation: int = Image.NEAREST,\n",
      "/home/aditya-ws/anaconda3/envs/scratch/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:328: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  interpolation: int = Image.BICUBIC,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default torch device to: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:03:22,441\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default torch device to: cuda\n",
      "(4, 84, 84) 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 444/1000000 [00:29<18:19:24, 15.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000012vscode-remote?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame_idx \u001b[39min\u001b[39;00m trange(\u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39mMAX_FRAMES \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000012vscode-remote?line=32'>33</a>\u001b[0m     observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_states_()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000012vscode-remote?line=33'>34</a>\u001b[0m     action \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_action(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000012vscode-remote?line=34'>35</a>\u001b[0m     prev_observation\u001b[39m=\u001b[39mobservation\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000012vscode-remote?line=35'>36</a>\u001b[0m     \u001b[39m# observation, reward, done, _ = env.step(action)\u001b[39;00m\n",
      "\u001b[1;32m/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb Cell 9'\u001b[0m in \u001b[0;36mModel.get_action\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=91'>92</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=92'>93</a>\u001b[0m     \u001b[39m# X = torch.tensor([s], device=self.device, dtype=torch.float) \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=93'>94</a>\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(s, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m) \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=94'>95</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msample_noise()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=95'>96</a>\u001b[0m     a \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(X) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantile_weight)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000008vscode-remote?line=96'>97</a>\u001b[0m     \u001b[39m# return a.item()\u001b[39;00m\n",
      "\u001b[1;32m/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb Cell 7'\u001b[0m in \u001b[0;36mDuelingQRDQN.sample_noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000006vscode-remote?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madv1\u001b[39m.\u001b[39msample_noise()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000006vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madv2\u001b[39m.\u001b[39msample_noise()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval1\u001b[39m.\u001b[39;49msample_noise()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225753227d/home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/10.Quantile-Rainbow.ipynb#ch0000006vscode-remote?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval2\u001b[39m.\u001b[39msample_noise()\n",
      "File \u001b[0;32m~/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py:38\u001b[0m, in \u001b[0;36mNoisyLinear.sample_noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py?line=35'>36</a>\u001b[0m     epsilon_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features)\n\u001b[1;32m     <a href='file:///home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py?line=36'>37</a>\u001b[0m     epsilon_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features)\n\u001b[0;32m---> <a href='file:///home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py?line=37'>38</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_epsilon\u001b[39m.\u001b[39mcopy_(epsilon_out\u001b[39m.\u001b[39;49mger(epsilon_in))\n\u001b[1;32m     <a href='file:///home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py?line=38'>39</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_epsilon\u001b[39m.\u001b[39mcopy_(epsilon_out)\n\u001b[1;32m     <a href='file:///home/aditya-ws/workspace/personal/thirdparty/DeepRL-Tutorials/networks/layers.py?line=39'>40</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from morl import environments as envs\n",
    "from morl import core\n",
    "\n",
    "core.set_default_torch_device(\"cuda\")\n",
    "start=timer()\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "# env    = make_atari(env_id)\n",
    "# env    = wrap_deepmind(env, frame_stack=False)\n",
    "# env    = wrap_pytorch(env)\n",
    "# env = envpool.make_gym(\n",
    "#     \"Pong-v5\",\n",
    "#     num_envs=1,\n",
    "#     seed=0,\n",
    "#     episodic_life=True,\n",
    "#     reward_clip=True,\n",
    "#     stack_num=4,\n",
    "# )\n",
    "# print(env.observation_space.shape)\n",
    "# env = envs.GymEnv.make_ale_env(\"Pong\")\n",
    "env = envs.EnvPool.make_ale_env(\n",
    "    game=\"Pong\",\n",
    "    n_envs=1,\n",
    "    base_seed=0,\n",
    "    frame_stack=4,\n",
    ")\n",
    "model = Model(env=env, config=config)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "# observation = env.reset()\n",
    "for frame_idx in trange(1, config.MAX_FRAMES + 1):\n",
    "    observation = env.get_states_()\n",
    "    action = model.get_action(observation)\n",
    "    prev_observation=observation\n",
    "    # observation, reward, done, _ = env.step(action)\n",
    "    exp_ = env.step_(action)\n",
    "    observation, reward, done = exp_.next_states[0], exp_.rewards.item(), exp_.dones.item()\n",
    "    observation = None if done else observation\n",
    "\n",
    "    # model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    model.morl_update(exp_, frame_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        model.reset_hx()\n",
    "        # observation = env.reset()\n",
    "        observation = env.get_states_()\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        if np.mean(model.rewards[-10:]) > 19:\n",
    "            plot(frame_idx, model.rewards, None, None, timedelta(seconds=int(timer()-start)))\n",
    "            break\n",
    "\n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, model.rewards, None, None, timedelta(seconds=int(timer()-start)))\n",
    "\n",
    "model.save_w()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
